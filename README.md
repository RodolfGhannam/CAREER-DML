# CAREER-DML: A Framework for Causal Inference on Career Trajectories (v5.0)

**Author:** Rodolf Mikel Ghannam Neto

**Date:** February 16, 2026

---

## 1. Overview

CAREER-DML is a framework for estimating the causal effect of career transitions using sequential data. It combines Recurrent Neural Network (RNN) embeddings with Double/Debiased Machine Learning (DML) to control for high-dimensional confounding from career histories.

This repository contains the full implementation of the framework, the data generating process, and the analysis that led to two key scientific findings:

1.  **The Embedding Paradox:** A demonstration that causally-motivated embeddings (VIB) can be counterproductive in sequential career data, exhibiting higher bias than simpler predictive embeddings.
2.  **The Signal-to-Noise Frontier:** A characterization of the sample size limitations for detecting realistic, small-magnitude causal effects, providing a rigorous motivation for the use of large-scale administrative data.

This work was developed as a proof-of-concept for a PhD application to Copenhagen Business School.

## 2. Key Results (v5.0)

The main results are generated by the `main_expanded.py` script, which runs a semi-synthetic DGP (ATE=0.08, N=1,000) and produces a detailed gain decomposition table.

**Table 1: Gain Decomposition of ATE Estimation Methods**

| Method | Type | Sequential? | ATE | Bias | |Bias|% |
|:---|:---|:---:|:---:|:---:|:---:|
| 1. Heckman Two-Step | Parametric | No | 0.8365 | 0.7565 | 945.6% |
| 2. LASSO + DML | Semi-parametric | No | 0.0362 | -0.0438 | 54.8% |
| 3. Random Forest + DML | Non-parametric | No | 0.0260 | -0.0540 | 67.5% |
| 4. Static Embedding + DML | Embedding | No | 0.0032 | -0.0768 | 96.0% |
| 5. Predictive GRU + DML | Embedding | Yes | -0.0174 | -0.0974 | 121.8% |
| 6. Causal GRU VIB + DML | Embedding | Yes | -0.0485 | -0.1285 | 160.6% |
| 7. Debiased GRU + DML | Embedding | Yes | -0.0093 | -0.0893 | 111.6% |

**Key Insights:**
- All modern ML methods dramatically outperform the classical Heckman model.
- At N=1,000, the simpler LASSO and RF models are the best performers, as the signal is too weak for the more complex sequential models to provide an incremental benefit.
- The Embedding Paradox is confirmed: the VIB model has the highest bias among the embedding methods.

**Figure 1: Signal-to-Noise Frontier**

![MDE vs N](results/figures/power_analysis_sensitivity.png)

Our power analysis shows that a sample size of **N > 1,034** is required to reliably detect the true ATE of 0.08. This provides a clear, data-driven motivation for the next stage of this research using large-scale administrative data.

## 3. Repository Structure

- `main_expanded.py`: Main script to run the full v5.0 pipeline.
- `src/`: Core modules for the framework (DGP, embeddings, DML, validation, power analysis).
- `docs/`: All documentation, including:
  - `candidature/`: Motivation Letter, Research Proposal, CV.
  - `technical/`: Paper drafts and technical notes.
- `results/`: All outputs from the pipeline runs, including figures and raw text logs.
- `tests/`: A suite of 50 unit and integration tests.
- `data/`: Input data, including AIOE scores and NLSY79 calibration parameters.

## 4. How to Run

1.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
2.  Run the main expanded pipeline:
    ```bash
    python3 main_expanded.py
    ```
3.  Run the tests:
    ```bash
    pytest
    ```
